{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34daf96e-77e0-4363-bc08-eb94d399880e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7059c0b3-f880-491b-964b-374924c3f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 32.1272 - val_loss: 30.0547\n",
      "Epoch 2/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26.1678 - val_loss: 19.9620\n",
      "Epoch 3/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.0130 - val_loss: 9.7803\n",
      "Epoch 4/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9664 - val_loss: -5.1501\n",
      "Epoch 5/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -10.8507 - val_loss: -27.2699\n",
      "Epoch 6/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -37.4805 - val_loss: -61.0047\n",
      "Epoch 7/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -73.0837 - val_loss: -112.3236\n",
      "Epoch 8/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -135.0452 - val_loss: -189.6085\n",
      "Epoch 9/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -218.9073 - val_loss: -298.8603\n",
      "Epoch 10/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -329.0144 - val_loss: -451.6079\n",
      "Epoch 11/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -502.0337 - val_loss: -662.8550\n",
      "Epoch 12/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -730.6216 - val_loss: -947.4319\n",
      "Epoch 13/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -1024.9863 - val_loss: -1317.4210\n",
      "Epoch 14/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -1429.3286 - val_loss: -1789.5529\n",
      "Epoch 15/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -1913.5055 - val_loss: -2382.4426\n",
      "Epoch 16/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -2543.6560 - val_loss: -3117.4102\n",
      "Epoch 17/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -3296.8313 - val_loss: -4021.0269\n",
      "Epoch 18/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -4258.0728 - val_loss: -5117.5957\n",
      "Epoch 19/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -5366.2529 - val_loss: -6441.2651\n",
      "Epoch 20/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -6741.2974 - val_loss: -8023.0679\n",
      "Epoch 21/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -8511.3750 - val_loss: -9894.7764\n",
      "Epoch 22/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -10337.0068 - val_loss: -12100.0244\n",
      "Epoch 23/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -12611.8291 - val_loss: -14678.5117\n",
      "Epoch 24/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -15539.3838 - val_loss: -17681.5781\n",
      "Epoch 25/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -18777.3867 - val_loss: -21147.9805\n",
      "Epoch 26/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -21905.2422 - val_loss: -25134.0215\n",
      "Epoch 27/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: -26214.9160 - val_loss: -29692.3945\n",
      "Epoch 28/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -30932.6250 - val_loss: -34886.3828\n",
      "Epoch 29/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -36158.9180 - val_loss: -40779.1016\n",
      "Epoch 30/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -43228.7773 - val_loss: -47433.6641\n",
      "Epoch 31/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -49738.8984 - val_loss: -54928.4141\n",
      "Epoch 32/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -56540.3633 - val_loss: -63337.2188\n",
      "Epoch 33/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -64571.8047 - val_loss: -72742.1875\n",
      "Epoch 34/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -75054.2969 - val_loss: -83233.1406\n",
      "Epoch 35/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -86892.4297 - val_loss: -94867.8516\n",
      "Epoch 36/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -97001.2266 - val_loss: -107787.7578\n",
      "Epoch 37/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -113904.1250 - val_loss: -122048.2734\n",
      "Epoch 38/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -124505.5391 - val_loss: -137789.2500\n",
      "Epoch 39/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -143944.4062 - val_loss: -155101.2656\n",
      "Epoch 40/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -160707.5312 - val_loss: -174074.8750\n",
      "Epoch 41/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -182577.0469 - val_loss: -194868.7500\n",
      "Epoch 42/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: -203138.0625 - val_loss: -217614.3125\n",
      "Epoch 43/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -220567.5469 - val_loss: -242403.9219\n",
      "Epoch 44/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -250172.2656 - val_loss: -269418.3750\n",
      "Epoch 45/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -276937.3438 - val_loss: -298784.2812\n",
      "Epoch 46/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -295609.2500 - val_loss: -330652.9688\n",
      "Epoch 47/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -329846.3438 - val_loss: -365170.0312\n",
      "Epoch 48/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -367481.7500 - val_loss: -402494.3438\n",
      "Epoch 49/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -412523.5000 - val_loss: -442794.3750\n",
      "Epoch 50/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -455430.2500 - val_loss: -486314.0938\n",
      "Epoch 51/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -485500.5625 - val_loss: -533166.6250\n",
      "Epoch 52/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -548706.1250 - val_loss: -583555.6875\n",
      "Epoch 53/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -589667.7500 - val_loss: -637707.0625\n",
      "Epoch 54/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -649544.1875 - val_loss: -695817.1250\n",
      "Epoch 55/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -714099.3125 - val_loss: -758146.8750\n",
      "Epoch 56/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -759145.0625 - val_loss: -824870.0625\n",
      "Epoch 57/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -844332.0000 - val_loss: -896182.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -932248.5000 - val_loss: -972429.8125\n",
      "Epoch 59/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -960531.6875 - val_loss: -1053835.8750\n",
      "Epoch 60/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -1064862.3750 - val_loss: -1140464.7500\n",
      "Epoch 61/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1173787.2500 - val_loss: -1232961.1250\n",
      "Epoch 62/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1234918.2500 - val_loss: -1331525.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1384818.1250 - val_loss: -1436360.3750\n",
      "Epoch 64/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1471556.7500 - val_loss: -1547614.8750\n",
      "Epoch 65/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1590813.6250 - val_loss: -1665857.5000\n",
      "Epoch 66/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1711439.2500 - val_loss: -1791301.6250\n",
      "Epoch 67/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -1801039.3750 - val_loss: -1924376.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -1938510.2500 - val_loss: -2065558.7500\n",
      "Epoch 69/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -2078838.5000 - val_loss: -2215037.2500\n",
      "Epoch 70/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -2254080.5000 - val_loss: -2373162.2500\n",
      "Epoch 71/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -2328591.7500 - val_loss: -2540355.7500\n",
      "Epoch 72/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -2551906.7500 - val_loss: -2716890.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -2721792.5000 - val_loss: -2903492.2500\n",
      "Epoch 74/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -2983644.5000 - val_loss: -3100398.7500\n",
      "Epoch 75/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -3092242.0000 - val_loss: -3307823.2500\n",
      "Epoch 76/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -3389924.2500 - val_loss: -3528549.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: -3501779.0000 - val_loss: -3769036.7500\n",
      "Epoch 78/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -3948022.5000 - val_loss: -3998290.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -3996354.5000 - val_loss: -4252087.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -4239123.0000 - val_loss: -4519335.5000\n",
      "Epoch 81/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -4594863.5000 - val_loss: -4800141.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -4727320.0000 - val_loss: -5094874.5000\n",
      "Epoch 83/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -5080916.5000 - val_loss: -5404222.5000\n",
      "Epoch 84/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -5620199.5000 - val_loss: -5728688.5000\n",
      "Epoch 85/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -5853450.0000 - val_loss: -6068924.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -6038987.0000 - val_loss: -6425148.5000\n",
      "Epoch 87/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -6613150.0000 - val_loss: -6798339.5000\n",
      "Epoch 88/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -6894418.5000 - val_loss: -7189296.5000\n",
      "Epoch 89/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -7421122.5000 - val_loss: -7597651.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -7716901.0000 - val_loss: -8024877.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -8117272.0000 - val_loss: -8471701.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -8654760.0000 - val_loss: -8938069.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -8897076.0000 - val_loss: -9425633.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -9425157.0000 - val_loss: -9933968.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -10045421.0000 - val_loss: -10464746.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -10672405.0000 - val_loss: -11018359.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -11216431.0000 - val_loss: -11593513.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -11633412.0000 - val_loss: -12193626.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: -12453197.0000 - val_loss: -12819080.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: -13191315.0000 - val_loss: -66456396.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "scaled_df = pd.read_csv('../../data/kdd_train_scaled.csv')\n",
    "X_train = scaled_df.to_numpy()  # Convert DataFrame to NumPy array\n",
    "\n",
    "# Check for NaN values in the dataset\n",
    "if np.any(np.isnan(X_train)):\n",
    "    print(\"Warning: NaN values found in the dataset. Consider handling them before training.\")\n",
    "    # Optional: Handle NaN values if necessary (e.g., using imputation)\n",
    "\n",
    "# Define the latent space dimension\n",
    "latent_dim = 2\n",
    "\n",
    "# Encoder\n",
    "input_data = layers.Input(shape=(X_train.shape[1],))  # Use preprocessed data's feature shape\n",
    "h = layers.Dense(64, activation='relu')(input_data)\n",
    "h = layers.Dense(32, activation='relu')(h)\n",
    "\n",
    "# Latent variables\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "# Sampling function for the latent space\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_h = layers.Dense(32, activation='relu')\n",
    "decoder_mean = layers.Dense(X_train.shape[1], activation='sigmoid')  # Match the number of features in the output\n",
    "\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# Define custom layer for KL divergence loss\n",
    "class KLDivergenceLayer(layers.Layer):\n",
    "    \"\"\" Custom layer to compute the KL divergence loss \"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        self.add_loss(K.mean(kl_loss))  # Add KL divergence as a loss to the model\n",
    "        return inputs\n",
    "\n",
    "# Apply the custom KL divergence layer\n",
    "z_mean, z_log_var = KLDivergenceLayer()([z_mean, z_log_var])\n",
    "\n",
    "# Define the VAE model\n",
    "vae = models.Model(input_data, x_decoded_mean)\n",
    "\n",
    "# Custom VAE loss function (reconstruction loss)\n",
    "def vae_loss(input_data, x_decoded_mean):\n",
    "    # Binary Crossentropy reconstruction loss if data is scaled to [0, 1]\n",
    "    reconstruction_loss = losses.binary_crossentropy(input_data, x_decoded_mean)\n",
    "    reconstruction_loss *= X_train.shape[1]  # Scale by the number of features\n",
    "    return K.mean(reconstruction_loss)  # KL loss is added by the custom layer\n",
    "\n",
    "# Compile the model with a lower learning rate and gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)  # Reduced learning rate and clipping\n",
    "vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "\n",
    "# Train VAE with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)  # Stop early if no improvement\n",
    "vae.fit(X_train, X_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Save the trained model\n",
    "vae.save('../../models/vae_model.h5')\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eafb94-80e4-4037-80c5-266cdf962f1f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f95f64ac-9461-4820-ae1b-9afaa56ce32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "NaNs in X_test_reconstructions: 0\n",
      "Reconstruction range: Min=0.0, Max=1.0\n",
      "Reconstruction MSE: 2.7615975264846275\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "vae = tf.keras.models.load_model('../../models/vae_model.h5', custom_objects={'sampling': sampling}, compile=False)\n",
    "\n",
    "X_test = pd.read_csv('../../data/kdd_test_scaled.csv')\n",
    "\n",
    "# Generate reconstructions on the validation/test set (assuming X_test is available)\n",
    "X_test_reconstructions = vae.predict(X_test)\n",
    "\n",
    "X_test_reconstructions = vae.predict(X_test)\n",
    "\n",
    "# Check for NaNs in the reconstructed data\n",
    "print(f\"NaNs in X_test_reconstructions: {np.isnan(X_test_reconstructions).sum()}\")\n",
    "\n",
    "# Check reconstruction output range\n",
    "print(f\"Reconstruction range: Min={X_test_reconstructions.min()}, Max={X_test_reconstructions.max()}\")\n",
    "\n",
    "\n",
    "# Calculate reconstruction error (Mean Squared Error)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(X_test, X_test_reconstructions)\n",
    "print(f'Reconstruction MSE: {mse}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
